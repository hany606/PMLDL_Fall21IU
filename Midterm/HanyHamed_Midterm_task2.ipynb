{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HanyHamed_Midterm_task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GvKw0iVbwVT"
      },
      "source": [
        "# Importing all the libraries\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmUeC_SdhYXL",
        "outputId": "90b1e14c-fc48-4e89-dea2-a920011c7459"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktwUxOAaeBbb",
        "outputId": "05ffb121-71c7-4fa8-e1a8-67fbe2c0d2ae"
      },
      "source": [
        "!unzip 'drive/MyDrive/Task2-dataset.zip' "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/Task2-dataset.zip\n",
            "   creating: Task2-dataset/\n",
            "  inflating: Task2-dataset/x_test.npy  \n",
            "  inflating: Task2-dataset/x_train.npy  \n",
            "  inflating: Task2-dataset/y_test_age.npy  \n",
            "  inflating: Task2-dataset/y_test_gender.npy  \n",
            "  inflating: Task2-dataset/y_train_age.npy  \n",
            "  inflating: Task2-dataset/y_train_gender.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGimHyTfiUkv"
      },
      "source": [
        "prefix = \"Task2-dataset/\"\n",
        "x_train = np.load(prefix+\"x_train.npy\")\n",
        "y_train_age = np.load(prefix+\"y_train_age.npy\")\n",
        "y_train_gender = np.load(prefix+\"y_train_gender.npy\")\n",
        "\n",
        "x_test = np.load(prefix+\"x_test.npy\")\n",
        "y_test_age = np.load(prefix+\"y_test_age.npy\")\n",
        "y_test_gender = np.load(prefix+\"y_test_gender.npy\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY1rxJBplPD0",
        "outputId": "24e209e0-fc51-4e7f-a221-37a2f1008d97"
      },
      "source": [
        "y_train = np.zeros((y_train_gender.shape[0], 2))\n",
        "for i in range(y_train_gender.shape[0]):\n",
        "    y_train[i,1] = y_train_gender[i]\n",
        "    y_train[i,0] = y_train_age[i]\n",
        "     \n",
        "y_test = np.zeros((y_test_gender.shape[0], 2))\n",
        "for i in range(y_test_gender.shape[0]):\n",
        "    y_test[i,1] = y_test_gender[i]\n",
        "    y_test[i,0] = y_test_age[i]\n",
        "     \n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "# 1 age\n",
        "# 2 gender"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6637, 2)\n",
            "(6685, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw7-zt4wyGs5",
        "outputId": "25174b08-bab5-417f-d935-087edfc790ce"
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6637, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h6w_fkSixpz"
      },
      "source": [
        "batch_size = 128\n",
        "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
        "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vWqpEHwyUj1"
      },
      "source": [
        "# batch_size = 128\n",
        "\n",
        "# x_train = x_train.reshape((-1, 1, 128, 128,3))\n",
        "# x_train_tensor = torch.tensor(x_train)\n",
        "\n",
        "# y_train = y_train.reshape(y_train.shape[0],2)\n",
        "# y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "# x_test_tensor = torch.tensor(x_test)\n",
        "# y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "\n",
        "\n",
        "# train_transform = transforms.Compose([\n",
        "#                     ### Define augmentations \n",
        "#                     transforms.Resize((128,128)),\n",
        "#                     transforms.ToTensor(),\n",
        "#                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # https://discuss.pytorch.org/t/understanding-transform-normalize/21730\n",
        "# ])\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#                     ### Define augmentations \n",
        "#                     transforms.Resize((128,128)),\n",
        "#                     transforms.ToTensor(),\n",
        "#                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # https://discuss.pytorch.org/t/understanding-transform-normalize/21730\n",
        "# ])\n",
        "\n",
        "\n",
        "# # Source: https://stackoverflow.com/questions/55588201/pytorch-transforms-on-tensordataset\n",
        "# class CustomTensorDataset(Dataset):\n",
        "#     \"\"\"TensorDataset with support of transforms.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, tensors, transform=None):\n",
        "#         # assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "#         self.tensors = tensors\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         x = self.tensors[0][index]\n",
        "\n",
        "#         if self.transform:\n",
        "#             x = self.transform(x)\n",
        "\n",
        "#         y = self.tensors[1][index]\n",
        "\n",
        "#         return x, y\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.tensors[0].size(0)\n",
        "\n",
        "# train_data_transform = CustomTensorDataset(tensors=(x_train_tensor, y_train_tensor), transform=train_transform)\n",
        "# test_data_transform = CustomTensorDataset(tensors=(x_test_tensor, y_test_tensor), transform=test_transform)\n",
        "\n",
        "# # train_data_transform = train_transform(train_data)\n",
        "# # test_data_transform = train_transform(test_data)\n",
        "\n",
        "# train_loader = DataLoader(train_data_transform, shuffle=True, batch_size=batch_size)\n",
        "# test_loader = DataLoader(test_data_transform, shuffle=True, batch_size=batch_size) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqrNiSGmxkH8",
        "outputId": "0c0f14bc-ca6e-468b-a94b-d4d60360330b"
      },
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "    # print(i, data[1].shape)\n",
        "    print(i, data[0].shape, data[0].dtype)\n",
        "    break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([128, 128, 128, 3]) torch.uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwhULzrdhsPL",
        "outputId": "8dc2d67c-0609-4fe1-c205-e4de5d743b15"
      },
      "source": [
        "# Source: https://discuss.pytorch.org/t/equivalent-of-keras-globalmaxpooling1d/45770/3\n",
        "class GlobalMaxPooling1D(nn.Module):\n",
        "    def __init__(self, data_format='channels_last'):\n",
        "        super(GlobalMaxPooling1D, self).__init__()\n",
        "        self.data_format = data_format\n",
        "        self.step_axis = 1 if self.data_format == 'channels_last' else 2\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.max(input, axis=self.step_axis).values\n",
        "\n",
        "\n",
        "class BranchModel(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, n_layers=1, drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.bi_lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
        "        self.bi_lstm2 = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.flip_horizontal = transforms.Compose([\n",
        "                                                   transforms.RandomHorizontalFlip(p=1)\n",
        "                                                 ])\n",
        "        \n",
        "        self.global_max_pooling = GlobalMaxPooling1D()\n",
        "\n",
        "        self.fc1 = nn.Linear(1024,1)\n",
        "        self.fc2 = nn.Linear(1024,1)\n",
        "\n",
        "    def forward(self, x, hidden1, hidden2):\n",
        "        inp1 = x\n",
        "        inp2 = self.flip_horizontal(x)\n",
        "\n",
        "        inp1_f = inp1.permute(0,4,1,2,3)\n",
        "        inp1_f = torch.flatten(inp1_f, 2)\n",
        "        lstm1_out, hidden1 = self.bi_lstm1(inp1_f.float(), hidden1)\n",
        "        lstm1_out = self.global_max_pooling(lstm1_out)\n",
        "\n",
        "\n",
        "\n",
        "        inp2_f = inp2.permute(0,4,1,2,3)\n",
        "        inp2_f = torch.flatten(inp2_f, 2)\n",
        "        lstm2_out, hidden2 = self.bi_lstm1(inp2_f.float(), hidden2)\n",
        "        lstm2_out = self.global_max_pooling(lstm2_out)\n",
        "\n",
        "        concat = torch.cat([lstm1_out, lstm2_out], 1)\n",
        "\n",
        "        out1 = self.fc1(concat)\n",
        "        out2 = self.fc2(concat)\n",
        "\n",
        "        return out1, out2, hidden1, hidden2\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # print(self.n_layers, batch_size, self.hidden_dim)\n",
        "        # return torch.zeros((self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
        "        h0 = torch.zeros((self.n_layers*2, batch_size, self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.n_layers*2, batch_size, self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "\n",
        "EMBEDDING_DIM = 16384#128*128*3\n",
        "HIDDEN_DIM = 256\n",
        "N_LAYERS = 1\n",
        "clip = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BranchModel(EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS).to(device)\n",
        "# loss_function1 = nn.L1Loss()\n",
        "# loss_function2 = nn.BCELoss()\n",
        "\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "lr = 0.002\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = None"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dnO6JCShu83"
      },
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "def train(model,train_loader,loss_fn,optimizer,scheduler,device):\n",
        "    accuracy = 0\n",
        "    running_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    model.train()\n",
        "    hidden1, hidden2 = model.init_hidden(batch_size), model.init_hidden(batch_size)\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        images,labels = inputs.to(device) , labels.to(device)\n",
        "        images = images.unsqueeze(1)\n",
        " \n",
        "        model.zero_grad()\n",
        "        hidden1 = tuple([each.data for each in hidden1])\n",
        "        hidden2 = tuple([each.data for each in hidden2])\n",
        "        out1, out2, hidden1, hidde2 = model(images, hidden1, hidden2)\n",
        "        # print(type(hidden1))\n",
        "        outputs = torch.cat([out1, out2], 1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        # print(labels[:,1].shape)\n",
        "        # loss = loss_fn[0](out1.squeeze().float(), labels[:,0].float()) + loss_fn[1](out2.squeeze().float(), labels[:,1].float())*10\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()* images.size(0)\n",
        "        total += images.size(0)\n",
        "        preds = torch.argmax(out2.data, 1)\n",
        "        correct += (preds == labels[:,1]).sum().item()\n",
        "\n",
        "        if i % 300 == 0:   \n",
        "            print('[%d, %5d] Acc: %.3f loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, correct/total, running_loss /total))\n",
        "            running_loss = 0.0\n",
        "        # scheduler.step(loss/total)\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    hidden1, hidden2 = model.init_hidden(batch_size), model.init_hidden(batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(loader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "            images,labels = inputs.to(device) , labels.to(device) \n",
        "            hidden1 = tuple([each.data for each in hidden1])\n",
        "            hidden2 = tuple([each.data for each in hidden2])\n",
        "            out1, out2, hidden1, hidde2 = model(images, hidden1, hidden2)\n",
        "            # print(type(hidden1))\n",
        "            outputs = torch.cat([out1, out2], 1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            total += inputs.size(0)\n",
        "            preds = torch.argmax(out2.data, 1)\n",
        "            correct += (preds == labels[:,1]).sum().item()\n",
        "            epoch_acc += correct/total\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    print(\"Epoch %d Test Accuracy %.3f\" % (epoch, epoch_acc/ len(loader)))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIBVRipWwgzB",
        "outputId": "cef2f4c5-c374-4ef5-a355-abf78ff5b5f7"
      },
      "source": [
        "NUM_EPOCHS = 10\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, train_loader, loss_function, optimizer, scheduler, device)\n",
        "    # train(model, train_loader, [loss_function1,loss_function2], optimizer, scheduler, device)\n",
        "    evaluate(model, test_loader, loss_function, device)\n",
        "    print(\"------------------------------------------------------------------------------\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] Acc: 0.539 loss: 18.974\n",
            "Epoch 0 Test Accuracy 0.560\n",
            "------------------------------------------------------------------------------\n",
            "[2,     1] Acc: 0.516 loss: 8.113\n",
            "Epoch 1 Test Accuracy 0.557\n",
            "------------------------------------------------------------------------------\n",
            "[3,     1] Acc: 0.570 loss: 6.711\n",
            "Epoch 2 Test Accuracy 0.558\n",
            "------------------------------------------------------------------------------\n",
            "[4,     1] Acc: 0.570 loss: 6.834\n",
            "Epoch 3 Test Accuracy 0.556\n",
            "------------------------------------------------------------------------------\n",
            "[5,     1] Acc: 0.617 loss: 6.643\n",
            "Epoch 4 Test Accuracy 0.564\n",
            "------------------------------------------------------------------------------\n",
            "[6,     1] Acc: 0.484 loss: 6.799\n",
            "Epoch 5 Test Accuracy 0.556\n",
            "------------------------------------------------------------------------------\n",
            "[7,     1] Acc: 0.617 loss: 6.414\n",
            "Epoch 6 Test Accuracy 0.556\n",
            "------------------------------------------------------------------------------\n",
            "[8,     1] Acc: 0.594 loss: 6.115\n",
            "Epoch 7 Test Accuracy 0.560\n",
            "------------------------------------------------------------------------------\n",
            "[9,     1] Acc: 0.547 loss: 7.554\n",
            "Epoch 8 Test Accuracy 0.550\n",
            "------------------------------------------------------------------------------\n",
            "[10,     1] Acc: 0.492 loss: 7.367\n",
            "Epoch 9 Test Accuracy 0.557\n",
            "------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}